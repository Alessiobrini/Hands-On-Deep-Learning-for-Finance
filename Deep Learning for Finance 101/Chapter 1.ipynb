{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading....  0 MMM\n",
      "Downloading....  1 AXP\n",
      "Downloading....  2 AAPL\n",
      "Downloading....  3 BA\n",
      "Downloading....  4 CAT\n",
      "Downloading....  5 CVX\n",
      "Downloading....  6 CSCO\n",
      "Downloading....  7 KO\n",
      "Downloading....  8 XOM\n",
      "Downloading....  9 GS\n",
      "Downloading....  10 HD\n",
      "Downloading....  11 IBM\n",
      "Downloading....  12 INTC\n",
      "Downloading....  13 JNJ\n",
      "Downloading....  14 JPM\n",
      "Downloading....  15 MCD\n",
      "Downloading....  16 MRK\n",
      "Downloading....  17 MSFT\n",
      "Downloading....  18 NKE\n",
      "Downloading....  19 PFE\n",
      "Downloading....  20 PG\n",
      "Downloading....  21 TRV\n",
      "Downloading....  22 UNH\n",
      "Downloading....  23 UTX\n",
      "Downloading....  24 VZ\n",
      "Downloading....  25 V\n",
      "Downloading....  26 WMT\n",
      "Downloading....  27 WBA\n",
      "Downloading....  28 DIS\n",
      "                  MMM        AXP       AAPL         BA        CAT        CVX  \\\n",
      "Date                                                                           \n",
      "2008-03-19  57.557903  34.362938  16.079836  53.679874  51.212154  52.488274   \n",
      "2008-03-20  56.478733  37.619267  16.526253  54.666512  51.274673  53.334328   \n",
      "2008-03-24  56.956772  38.789234  17.302536  55.755455  52.809711  53.847111   \n",
      "2008-03-25  57.202991  38.871040  17.482340  55.470432  53.233425  54.186821   \n",
      "2008-03-26  56.971241  37.112000  17.988289  55.762768  53.733536  54.456013   \n",
      "\n",
      "                 CSCO         KO        XOM          GS  ...        PFE  \\\n",
      "Date                                                     ...              \n",
      "2008-03-19  18.942581  18.267801  57.990337  142.177139  ...  12.750079   \n",
      "2008-03-20  19.174816  18.596844  58.381847  153.398285  ...  12.737708   \n",
      "2008-03-24  19.848293  18.639492  59.034348  152.757812  ...  12.750079   \n",
      "2008-03-25  19.933451  18.709562  58.519188  153.398285  ...  12.855244   \n",
      "2008-03-26  19.182554  18.630360  59.247253  149.854279  ...  12.799568   \n",
      "\n",
      "                   PG        TRV        UNH        UTX         VZ          V  \\\n",
      "Date                                                                           \n",
      "2008-03-19  46.832161  34.599716  30.380360  51.762524  18.069862  11.164822   \n",
      "2008-03-20  47.909573  35.628536  30.183817  51.950722  18.579092  12.716040   \n",
      "2008-03-24  48.323936  35.553982  30.517092  52.951942  19.016306  11.803095   \n",
      "2008-03-25  48.026962  35.606159  30.149624  52.816448  18.980297  12.498672   \n",
      "2008-03-26  48.096046  35.449600  29.141218  52.402401  18.589375  12.638976   \n",
      "\n",
      "                  WMT        WBA        DIS  \n",
      "Date                                         \n",
      "2008-03-19  38.291595  27.982969  26.503475  \n",
      "2008-03-20  40.139065  28.290640  27.063406  \n",
      "2008-03-24  40.440685  29.698250  27.182184  \n",
      "2008-03-25  40.003334  29.236744  27.216118  \n",
      "2008-03-26  39.890221  29.198273  26.944633  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# Dow Jones 30\n",
    "symbols_table = pd.read_html(\"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average#Components\",\n",
    " header=0)[2]\n",
    "symbols = list(symbols_table.loc[:, \"Symbol\"])\n",
    "index_symbol = ['^DJI']\n",
    "\n",
    "\n",
    "# Dates\n",
    "start_date = '2008-01-01'\n",
    "end_date = '2017-12-31'\n",
    "\n",
    "\n",
    "# Download the data\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    symbols[i]=symbols[i].replace(u'\\xa0',u'').replace(\"NYSE:\",\"\")\n",
    "\n",
    "symbols.remove('DOW') # DOW data are unvailable on yahoo\n",
    "\n",
    "for i in range(len(symbols)):\n",
    "    print('Downloading.... ', i, symbols[i])\n",
    "\n",
    "    # User pandas_reader.data.DataReader to load the desired data. As simple as that.\n",
    "    data[symbols[i]] = pdr.DataReader(symbols[i], \"yahoo\", start_date, end_date)['Adj Close']\n",
    "    data_index = pdr.DataReader(index_symbol, \"yahoo\", start_date, end_date)['Adj Close']\n",
    "\n",
    "# Remove the missing the data from the dataframe\n",
    "data = data.dropna()\n",
    "data_index = data_index.dropna()\n",
    "\n",
    "# Save the data\n",
    "data.to_csv('dj30_10y.csv', sep=',', encoding='utf-8')\n",
    "data_index.to_csv('dj30_index_10y.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2465/2465 [==============================] - 1s 501us/step - loss: 0.0298\n",
      "Epoch 2/20\n",
      "2465/2465 [==============================] - 1s 450us/step - loss: 0.0104\n",
      "Epoch 3/20\n",
      "2465/2465 [==============================] - 1s 448us/step - loss: 0.0058\n",
      "Epoch 4/20\n",
      "2465/2465 [==============================] - 1s 453us/step - loss: 0.0044\n",
      "Epoch 5/20\n",
      "2465/2465 [==============================] - 1s 450us/step - loss: 0.0037\n",
      "Epoch 6/20\n",
      "2465/2465 [==============================] - 1s 450us/step - loss: 0.0033\n",
      "Epoch 7/20\n",
      "2465/2465 [==============================] - 1s 449us/step - loss: 0.0030\n",
      "Epoch 8/20\n",
      "2465/2465 [==============================] - 1s 450us/step - loss: 0.0027\n",
      "Epoch 9/20\n",
      "2465/2465 [==============================] - 1s 448us/step - loss: 0.0026\n",
      "Epoch 10/20\n",
      "2465/2465 [==============================] - 1s 451us/step - loss: 0.0024\n",
      "Epoch 11/20\n",
      "2465/2465 [==============================] - 1s 448us/step - loss: 0.0023\n",
      "Epoch 12/20\n",
      "2465/2465 [==============================] - 1s 453us/step - loss: 0.0022\n",
      "Epoch 13/20\n",
      "2465/2465 [==============================] - 1s 452us/step - loss: 0.0022\n",
      "Epoch 14/20\n",
      "2465/2465 [==============================] - 1s 469us/step - loss: 0.0021\n",
      "Epoch 15/20\n",
      "2465/2465 [==============================] - 1s 450us/step - loss: 0.0021\n",
      "Epoch 16/20\n",
      "2465/2465 [==============================] - 1s 455us/step - loss: 0.0020\n",
      "Epoch 17/20\n",
      "2465/2465 [==============================] - 1s 450us/step - loss: 0.0020\n",
      "Epoch 18/20\n",
      "2465/2465 [==============================] - 1s 451us/step - loss: 0.0020\n",
      "Epoch 19/20\n",
      "2465/2465 [==============================] - 1s 451us/step - loss: 0.0020\n",
      "Epoch 20/20\n",
      "2465/2465 [==============================] - 1s 451us/step - loss: 0.0020\n",
      "2465/2465 [==============================] - 0s 20us/step\n",
      "test loss: 0.0023412757249016308\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('dj30_10y.csv', sep=',', engine='python')\n",
    "assets = data.columns.values[1:].tolist()\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "# Load index\n",
    "index = pd.read_csv('dj30_index_10y.csv', sep=',', engine='python')\n",
    "index = index.iloc[-data.values.shape[0]:, 1:]\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler([0.1,0.9])\n",
    "data_X = scaler.fit_transform(data)\n",
    "scaler_index = MinMaxScaler([0.1,0.9])\n",
    "index = scaler_index.fit_transform(index)\n",
    "\n",
    "# Number of components\n",
    "N_COMPONENTS = 3\n",
    "\n",
    "## Autoencoder - Keras\n",
    "# Network hyperparameters\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "# Create model\n",
    "input = Input(shape=(n_inputs,))\n",
    "# Encoder\n",
    "encoded = Dense(n_core, activation='sigmoid')(input)\n",
    "# Decoder\n",
    "decoded = Dense(n_outputs, activation='sigmoid')(encoded)\n",
    "\n",
    "# define model\n",
    "autoencoder = Model(input, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Testing in-sample\n",
    "X_train = data_X\n",
    "X_test = data_X\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "\n",
    "# Fit the model\n",
    "history = autoencoder.fit(X_train,\\\n",
    "                          X_train,\\\n",
    "                          epochs=epochs,\\\n",
    "                          batch_size=1,\\\n",
    "                          shuffle=True,\\\n",
    "                          verbose=1)\n",
    "\n",
    "# Make AE predictions\n",
    "y_pred_AE_keras = autoencoder.predict(X_test)\n",
    "\n",
    "print('test loss: '+str(autoencoder.evaluate(y_pred_AE_keras, X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "initializer = tf.initializers.glorot_normal()\n",
    "w1 = tf.Variable(initializer([n_inputs, n_core]))\n",
    "w2 = tf.transpose(w1)\n",
    "b1 = tf.Variable(tf.zeros([n_core]))\n",
    "b2 = tf.Variable(tf.zeros([n_outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w1), b1))\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cascabel/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 0 -> Loss: 0.002259\n",
      "Epoch: 1 -> Loss: 0.024630\n",
      "Epoch: 2 -> Loss: 0.001215\n",
      "Epoch: 3 -> Loss: 0.004160\n",
      "Epoch: 4 -> Loss: 0.003141\n",
      "Epoch: 5 -> Loss: 0.002592\n",
      "Epoch: 6 -> Loss: 0.002864\n",
      "Epoch: 7 -> Loss: 0.003957\n",
      "Epoch: 8 -> Loss: 0.004429\n",
      "Epoch: 9 -> Loss: 0.002020\n",
      "Epoch: 10 -> Loss: 0.002364\n",
      "Epoch: 11 -> Loss: 0.001039\n",
      "Epoch: 12 -> Loss: 0.001611\n",
      "Epoch: 13 -> Loss: 0.001949\n",
      "Epoch: 14 -> Loss: 0.008038\n",
      "Epoch: 15 -> Loss: 0.003026\n",
      "Epoch: 16 -> Loss: 0.000918\n",
      "Epoch: 17 -> Loss: 0.002105\n",
      "Epoch: 18 -> Loss: 0.001802\n",
      "Epoch: 19 -> Loss: 0.003868\n",
      "Test Error: 0.002849\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_inputs])\n",
    "Y = tf.placeholder(\"float\", [None, n_inputs])\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "y_true = X\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "mse = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(mse)\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the network\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training\n",
    "    for i in range(epochs):\n",
    "        X_train1 = shuffle(X_train)\n",
    "        for j in range(X_train.shape[0] // batch_size):\n",
    "            batch_y = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            batch_x = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            _, loss_value = sess.run([optimizer, mse], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # Display loss\n",
    "        print('Epoch: %i -> Loss: %f' % (i, loss_value))\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_AE_tf = sess.run(decoder_op, feed_dict={X: X_train, Y: X_train})\n",
    "    print('Test Error: %f' % tf.losses.mean_squared_error(X_train, y_pred_AE_tf).eval())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
