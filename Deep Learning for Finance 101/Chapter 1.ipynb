{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# Dow Jones 30\n",
    "symbols_table = pd.read_html(\"https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average#Components\",\n",
    " header=0)[2]\n",
    "symbols = list(symbols_table.loc[:, \"Symbol\"])\n",
    "index_symbol = ['^DJI']\n",
    "\n",
    "\n",
    "# Dates\n",
    "start_date = '2008-01-01'\n",
    "end_date = '2017-12-31'\n",
    "\n",
    "# Download the data\n",
    "data = pd.DataFrame()\n",
    "for i in range(len(symbols)):\n",
    "    print('Downloading.... ', i, symbols[i])\n",
    "\n",
    "    # User pandas_reader.data.DataReader to load the desired data. As simple as that.\n",
    "    data[symbols[i]] = pdr.DataReader(symbols[i], 'yahoo', start_date, end_date)['Adj Close']\n",
    "    data_index = pdr.DataReader(index_symbol, 'yahoo', start_date, end_date)['Adj Close']\n",
    "\n",
    "# Remove the missing the data from the dataframe\n",
    "data = data.dropna()\n",
    "data_index = data_index.dropna()\n",
    "\n",
    "# Save the data\n",
    "data.to_csv('dj30_10y.csv', sep=',', encoding='utf-8')\n",
    "data_index.to_csv('dj30_index_10y.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('dj30_10y.csv', sep=',', engine='python')\n",
    "assets = data.columns.values[1:].tolist()\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "# Load index\n",
    "index = pd.read_csv('dj30_index_10y.csv', sep=',', engine='python')\n",
    "index = index.iloc[-data.values.shape[0]:, 1:]\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler([0.1,0.9])\n",
    "data_X = scaler.fit_transform(data)\n",
    "scaler_index = MinMaxScaler([0.1,0.9])\n",
    "index = scaler_index.fit_transform(index)\n",
    "\n",
    "# Number of components\n",
    "N_COMPONENTS = 3\n",
    "\n",
    "## Autoencoder - Keras\n",
    "# Network hyperparameters\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "# Create model\n",
    "input = Input(shape=(n_inputs,))\n",
    "# Encoder\n",
    "encoded = Dense(n_core, activation='sigmoid')(input)\n",
    "# Decoder\n",
    "decoded = Dense(n_outputs, activation='sigmoid')(encoded)\n",
    "\n",
    "# define model\n",
    "autoencoder = Model(input, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Testing in-sample\n",
    "X_train = data_X\n",
    "X_test = data_X\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "\n",
    "# Fit the model\n",
    "history = autoencoder.fit(X_train,\\\n",
    "                          X_train,\\\n",
    "                          epochs=epochs,\\\n",
    "                          batch_size=1,\\\n",
    "                          shuffle=True,\\\n",
    "                          verbose=1)\n",
    "\n",
    "# Make AE predictions\n",
    "y_pred_AE_keras = autoencoder.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow - Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "initializer = tf.initializers.glorot_normal()\n",
    "w1 = tf.Variable(initializer([n_inputs, n_core]))\n",
    "w2 = tf.transpose(w1)\n",
    "b1 = tf.Variable(tf.zeros([n_core]))\n",
    "b2 = tf.Variable(tf.zeros([n_outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w1), b1))\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_inputs])\n",
    "Y = tf.placeholder(\"float\", [None, n_inputs])\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "y_true = X\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "mse = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(mse)\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the network\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training\n",
    "    for i in range(epochs):\n",
    "        X_train1 = shuffle(X_train)\n",
    "        for j in range(X_train.shape[0] // batch_size):\n",
    "            batch_y = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            batch_x = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            _, loss_value = sess.run([optimizer, mse], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # Display loss\n",
    "        print('Epoch: %i -> Loss: %f' % (i, loss_value))\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_AE_tf = sess.run(decoder_op, feed_dict={X: X_train, Y: X_train})\n",
    "    print('Test Error: %f' % tf.losses.mean_squared_error(X_train, y_pred_AE_tf).eval())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
